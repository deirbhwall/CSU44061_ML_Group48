# -*- coding: utf-8 -*-
"""Machine Learning Week 5

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/deirbhwall/982c88f8d5c1d522850c8c3dae874030/machine-learning-week-5.ipynb
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

tweets = pd.read_csv("TextNameGender.csv",names=['Tweet','Name','Gender'],skiprows=1)

tweets = tweets.sample(frac=1).reset_index(drop=True)

tweets.head()

gender_dataset = tweets[["Tweet","Gender"]]
person_dataset = tweets[["Tweet","Name"]]
person_dataset.head()

gender_dataset.head()

X_tweet = np.array(gender_dataset["Tweet"])

y_gender = np.array(gender_dataset["Gender"])
y_person = np.array(person_dataset["Name"])

#Tokenizing

from nltk.tokenize import TweetTokenizer
tweet_tokenizer = TweetTokenizer()

X_tokenize = list()

for tweet in X_tweet:
    X_tokenize.append(tweet_tokenizer.tokenize(tweet))

#stop words + punctuation removal

import nltk 
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = stopwords.words("english")

import string
punctuations = list(string.punctuation)

stop_words = stop_words + punctuations

stop_words = stop_words + list('’“‚—')
#stop_words

#Lemmatisation
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

import nltk
nltk.download('wordnet')

#Removing stop word and lemmatizing
X_clean = []
for i in range(len(X_tokenize)):
    temp = []
    for word in X_tokenize[i]:
        if word not in stop_words:
            clean_word = lemmatizer.lemmatize(word)
            temp.append(clean_word.lower())
    X_clean.append(temp)
#X_clean[:20]

X_document = []
for doc in X_clean:
    X_document.append(" ".join(doc))
X_document[:5]

"""Cross-validation for the hyperparameters of CountVectorization"""

def error_plots(x_range,mean_error,std_error,x_name="",title_text=""):
    '''
    This function plot errorbars
    '''
    plt.xlabel(x_name,fontweight='bold')
    plt.ylabel("Accuracy Score",fontweight='bold')
    plt.title("Error Bar for "+title_text,fontweight='bold')
    plt.errorbar(x_range,mean_error,std_error)
    plt.show()

#Cross-Validation for Logistic Regression C Value
from sklearn.linear_model import LogisticRegression

def cross_val_LR(X,y):
    '''
    This fucntion takes X data and y data, 
    trains Logistic regression model over the data
    and return the maximum mean accuracy,
    standard deviation associated with that maximum accuracy, 
    and the best model parameter value.
    '''

    C_range = [0.01,0.1,1,10,100]

    mean_accuracy=[]
    std_accuracy=[]

    for C_value in C_range:
        temp=[]
        for train,test in kf.split(X):
            model = LogisticRegression(C=C_value, max_iter=2000).fit(X[train],y[train])
            prediction = model.predict(X[test])
            temp.append(accuracy_score(y[test],prediction))
        mean_accuracy.append(np.array(temp).mean())
        std_accuracy.append(np.array(temp).std())

    return (C_range[mean_accuracy.index(max(mean_accuracy))],
            max(mean_accuracy),
            std_accuracy[mean_accuracy.index(max(mean_accuracy))])

#Cross validation for KNN
from sklearn.neighbors import KNeighborsClassifier

def cross_val_KNN(X,y):
    '''
    This fucntion takes X data and y data, 
    trains KNN model over the data
    and return the maximum mean accuracy,
    standard deviation associated with that maximum accuracy, 
    and the best model parameter value.
    '''
    K_range = [2,3,4,5,6,7,8,9,10]

    mean_accuracy=[]
    std_accuracy=[]
    for K_value in K_range:
        temp=[]
        for train,test in kf.split(X):
            model = KNeighborsClassifier(n_neighbors=K_value).fit(X[train],y[train])
            prediction = model.predict(X[test])
            temp.append(accuracy_score(y[test],prediction))
        mean_accuracy.append(np.array(temp).mean())
        std_accuracy.append(np.array(temp).std())

    return ((K_range[mean_accuracy.index(max(mean_accuracy))],
             max(mean_accuracy),
            std_accuracy[mean_accuracy.index(max(mean_accuracy))]))

#Cross-validation for SVM C Value
from sklearn.svm import LinearSVC

def cross_val_SVC(X,y):
    '''
    This fucntion takes X data and y data, 
    trains SVC over the data
    and return the maximum mean accuracy,
    standard deviation associated with that maximum accuracy, 
    and the best model parameter value.
    '''

    C_range = [0.01,0.1,1,10,100]

    mean_accuracy=[]
    std_accuracy=[]

    for C_value in C_range:
        temp=[]
        for train,test in kf.split(X):
            model = LinearSVC(C=C_value,max_iter=2000).fit(X[train],y[train])
            prediction = model.predict(X[test])
            temp.append(accuracy_score(y[test],prediction))
        mean_accuracy.append(np.array(temp).mean())
        std_accuracy.append(np.array(temp).std())
    
    return ((C_range[mean_accuracy.index(max(mean_accuracy))],
             max(mean_accuracy),
            std_accuracy[mean_accuracy.index(max(mean_accuracy))]))

def accuracy_errorbar(accuracy_list,x_range,x_name, model_name = ""):
    '''
    This function doesn't plot errorbars but calls the error_bar function
    it is mainly used to segregate best parameters, max accuracy with it's
    standrad deviation
    '''
    accuracy_list = np.array(accuracy_list)
    (parameter_range, max_accuracy, max_std )= (accuracy_list[:,0],
                                                accuracy_list[:,1],
                                                accuracy_list[:,2])

    error_plots(x_range,max_accuracy, max_std,x_name+" range", model_name)

    max_accuracy_index = list(max_accuracy).index(max(max_accuracy))
    print("For "+model_name+"\n\tbest model parameter value = {}\n\tbest ".
          format(parameter_range[max_accuracy_index])+x_name+" value = {}\n\twith accuracy score = {}"
    .format(x_range[max_accuracy_index],max(max_accuracy)))

#K-Fold Cross-validation 
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import CountVectorizer

#Using 5 splits for KFold
kf = KFold(n_splits = 5)

#cross validation for Max_df
max_df_range = [0.4,0.5,0.6,0.7,0.8,0.9,1]


max_accuracy_LR = []
max_accuracy_KNN = []
max_accuracy_SVC = []

for max_df_value in max_df_range:
    count_vector = CountVectorizer(max_features=5000, ngram_range=(1,2), max_df = max_df_value)
    X_cross_features = count_vector.fit_transform(X_document)
    X_cross_features.todense()
    max_accuracy_LR.append(cross_val_LR(X_cross_features,y_gender))
    max_accuracy_KNN.append(cross_val_KNN(X_cross_features,y_gender))
    max_accuracy_SVC.append(cross_val_SVC(X_cross_features,y_gender))

accuracy_errorbar(max_accuracy_LR,max_df_range,"Max_df", "Logistic Regression : Max_df")
accuracy_errorbar(max_accuracy_KNN,max_df_range,"Max_df", "KNN : Max_df")
accuracy_errorbar(max_accuracy_SVC,max_df_range,"Max_df", "SVC : Max_df")

#cross validation for Min_df
min_df_range = [0,0.1,0.2,0.3,0.4]
min_accuracy_LR = []
min_accuracy_KNN = []
min_accuracy_SVC = []

for min_df_value in min_df_range:
    count_vector = CountVectorizer(max_features=5000, ngram_range=(1,2), min_df = min_df_value)
    X_cross_features = count_vector.fit_transform(X_document)
    X_cross_features.todense()
    min_accuracy_LR.append(cross_val_LR(X_cross_features,y_gender))
    min_accuracy_KNN.append(cross_val_KNN(X_cross_features,y_gender))
    min_accuracy_SVC.append(cross_val_SVC(X_cross_features,y_gender))

accuracy_errorbar(min_accuracy_LR,min_df_range,"Min_df", "Logistic Regression : Min_df")
accuracy_errorbar(min_accuracy_KNN,min_df_range,"Min_df", "KNN : Min_df")
accuracy_errorbar(min_accuracy_SVC,min_df_range,"Min_df", "SVC : Min_df")

#cross validation for Max_features
max_features_range = [100,500,1000,2000,3000,5000]


features_accuracy_LR = []
features_accuracy_KNN = []
features_accuracy_SVC = []

for max_features_value in max_features_range:
    count_vector = CountVectorizer(max_features=max_features_value, ngram_range=(1,2))
    X_cross_features = count_vector.fit_transform(X_document)
    X_cross_features.todense()
    features_accuracy_LR.append(cross_val_LR(X_cross_features,y_gender))
    features_accuracy_KNN.append(cross_val_KNN(X_cross_features,y_gender))
    features_accuracy_SVC.append(cross_val_SVC(X_cross_features,y_gender))

accuracy_errorbar(features_accuracy_LR,max_features_range,"Max_features", "Logistic Regression : max_features")
accuracy_errorbar(features_accuracy_KNN,max_features_range,"Max_features", "KNN : max_features")
accuracy_errorbar(features_accuracy_SVC,max_features_range,"Max_features", "SVC : max_features")

#Cross-validation for ngram_range

max_ngram_range=[1,2,3]

ngram_accuracy_LR = []
ngram_accuracy_KNN = []
ngram_accuracy_SVC = []

for max_ngram_value in max_ngram_range:
    count_vector = CountVectorizer(max_features=2000, ngram_range=(1,max_ngram_value))
    X_cross_features = count_vector.fit_transform(X_document)
    X_cross_features.todense()
    ngram_accuracy_LR.append(cross_val_LR(X_cross_features,y_gender))
    ngram_accuracy_KNN.append(cross_val_KNN(X_cross_features,y_gender))
    ngram_accuracy_SVC.append(cross_val_SVC(X_cross_features,y_gender))

accuracy_errorbar(ngram_accuracy_LR,max_ngram_range,"ngram", "Logistic Regression : ngram_range")
accuracy_errorbar(ngram_accuracy_KNN,max_ngram_range,"ngram", "KNN : ngram_range")
accuracy_errorbar(ngram_accuracy_SVC,max_ngram_range,"ngram", "SVC : ngram_range")

"""**Cross-validation for Hyperparameters of the models**

Best hyperparameters for countVector

ngram - (1,2)

max_features - 5000

max_df = 0.6

min_df = 0
"""

#Vectorizing our dataset
from sklearn.feature_extraction.text import CountVectorizer

count_vec = CountVectorizer(max_features = 5000, ngram_range = (1,2), min_df = 0, max_df=0.6)

X_features = count_vec.fit_transform(X_document)
X_features.todense()

#Error-bars for Logistic Regression C Value
from sklearn.linear_model import LogisticRegression

def error_bar_LR(X,y):
    '''
    This fucntion takes X data and y data, 
    trains Logistic regression model over the data
    then plots errorbar of mean accuracy and standard deviation in accuracy 
    values of different splits.
    It then gives the best parameter value with an accuracy score
    '''

    C_range = [0.01,0.1,1,10,100]

    mean_accuracy=[]
    std_accuracy=[]

    for C_value in C_range:
        temp=[]
        for train,test in kf.split(X):
            model = LogisticRegression(C=C_value, max_iter=2000).fit(X[train],y[train])
            prediction = model.predict(X[test])
            temp.append(accuracy_score(y[test],prediction))
        mean_accuracy.append(np.array(temp).mean())
        std_accuracy.append(np.array(temp).std())

    #Error bar for Logistic Regression
    error_plots(C_range,mean_accuracy,std_accuracy,"C range","for Logistic Regression")
    error_plots(C_range[:4],mean_accuracy[:4],std_accuracy[:4],"C range","for Logistic Regression")

    print("For Logistic Regression\n\tbest value of C - {}\n\twith accuracy = {}".
          format(C_range[mean_accuracy.index(max(mean_accuracy))],max(mean_accuracy)))

error_bar_LR(X_features,y_gender)

#Error bar for KNN
from sklearn.neighbors import KNeighborsClassifier

def error_bar_KNN(X,y):
    '''
    This fucntion takes X data and y data, 
    trains KNN model over the data
    then plots errorbar of mean accuracy and standard deviation in accuracy 
    values of different splits.
    It then gives the best parameter value with an accuracy score
    '''
    K_range = [2,3,4,5,6,7,8,9,10]

    mean_accuracy=[]
    std_accuracy=[]
    for K_value in K_range:
        temp=[]
        for train,test in kf.split(X):
            model = KNeighborsClassifier(n_neighbors=K_value).fit(X[train],y[train])
            prediction = model.predict(X[test])
            temp.append(accuracy_score(y[test],prediction))
        mean_accuracy.append(np.array(temp).mean())
        std_accuracy.append(np.array(temp).std())

    #Error bar for KNN
    error_plots(K_range,mean_accuracy,std_accuracy,"C range","for KNN")

    print("For KNN\n\tbest value of K - {}\n\twith accuracy = {}".
          format(K_range[mean_accuracy.index(max(mean_accuracy))],max(mean_accuracy)))
    
error_bar_KNN(X_features,y_gender)

#Error bar for SVM C Value
from sklearn.svm import LinearSVC

def error_bar_SVC(X,y):
    '''
    This fucntion takes X data and y data, 
    trains SVC model over the data
    then plots errorbar of mean accuracy and standard deviation in accuracy 
    values of different splits.
    It then gives the best parameter value with an accuracy score
    '''

    C_range = [0.01,0.1,1,10,100]

    mean_accuracy=[]
    std_accuracy=[]

    for C_value in C_range:
        temp=[]
        print(C_value)
        for train,test in kf.split(X):
            model = LinearSVC(C=C_value,max_iter=1000).fit(X[train],y[train])
            prediction = model.predict(X[test])
            temp.append(accuracy_score(y[test],prediction))
        mean_accuracy.append(np.array(temp).mean())
        std_accuracy.append(np.array(temp).std())
    
    #Error bar for SVC
    error_plots(C_range,mean_accuracy,std_accuracy,"C range","for SVC")
    error_plots(C_range[:3],mean_accuracy[:3],std_accuracy[:3],"C range","for SVC")

    print("For SVC\n\tbest value of C - {}\n\twith accuracy = {}".
          format(C_range[mean_accuracy.index(max(mean_accuracy))],max(mean_accuracy)))
    
error_bar_SVC(X_features,y_gender)

"""**Model Training**"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

X_train,X_test,y_train,y_test = train_test_split(X_features,y_gender,test_size=0.2)

#Logistic Regression

from sklearn.linear_model import LogisticRegression
LR_model = LogisticRegression(C=1, max_iter=2000)
LR_model.fit(X_train,y_train)

LR_predict = LR_model.predict(X_test)

print("Logistic Regression accuracy score : {}".format(accuracy_score(y_test,LR_predict)))

print("Confusion matrix for Logistic Regression model - ")
print(confusion_matrix(y_test,LR_predict))

#Did we over-fit

LR_model = LogisticRegression(C=1, max_iter=2000)
LR_model.fit(X_train,y_train)

LR_train_predict = LR_model.predict(X_train)

print("Logistic Regression training accuracy : {}".format(accuracy_score(y_train,LR_train_predict)))

#KNN

from sklearn.neighbors import KNeighborsClassifier

KNeighbor = KNeighborsClassifier(n_neighbors=2, weights='uniform')
KNeighbor.fit(X_train,y_train)

KNN_predict = KNeighbor.predict(X_test)

print("KNN : {}".format(accuracy_score(y_test,KNN_predict)))

print("Confusion matrix for KNN model - ")
print(confusion_matrix(y_test,KNN_predict))

#SVM
from sklearn.svm import LinearSVC

SVC_model = LinearSVC(C=0.1,max_iter=2000) 
SVC_model.fit(X_train,y_train)

SVC_predict = SVC_model.predict(X_test)

print("SVC : {}".format(accuracy_score(y_test,SVC_predict)))

print("Confusion matrix for SVC model - ")
print(confusion_matrix(y_test,SVC_predict))

#Dummy classifier
from sklearn.dummy import DummyClassifier
dummy = DummyClassifier(strategy="most_frequent")

dummy.fit(X_train,y_train)
dummy_prediction = dummy.predict(X_test)

print("Dummy classifier accuracy score : {}".format(accuracy_score(y_test,dummy_prediction)))

print("Confusion matrix for Dummy classifier - ")
print(confusion_matrix(y_test,dummy_prediction))

def plot_roc(fpr,tpr,parameter):
    '''
    Creates ROC curves
    '''
    plt.plot(fpr,tpr,color='navy')
    plt.xlabel("False Positive Rate",fontweight='bold')
    plt.ylabel("True Positive Rate",fontweight='bold')
    plt.title("ROC Curve for "+parameter,fontweight='bold')
    plt.plot([0,1],[0,1],color='red',linestyle='--')
    plt.show()

from sklearn.metrics import roc_curve
from sklearn.model_selection import cross_val_score

LR_fpr,LR_tpr,_ = roc_curve(y_test,LR_model.decision_function(X_test), pos_label=['Male'])
plot_roc(LR_fpr,LR_tpr,"Logistic Regression")


y_scores = KNeighbor.predict_proba(X_test)
KNN_fpr,KNN_tpr,_ = roc_curve(y_test,y_scores[:,1], pos_label=['Male'])      #y_scores col 1 -> prob estimate of + class
plot_roc(KNN_fpr,KNN_tpr,"K Neighbors")

SVC_fpr,SVC_tpr,_ = roc_curve(y_test,SVC_model.decision_function(X_test), pos_label=['Male'])
plot_roc(SVC_fpr, SVC_tpr, "Support Vector Classifier")

y_scores = dummy.predict_proba(X_test)
dummy_fpr,dummy_tpr,_ = roc_curve(y_test,y_scores[:,1],pos_label=['Male'])
plt.scatter(dummy_fpr,dummy_tpr,color='navy')
plt.xlabel("False Positive Rate",fontweight='bold')
plt.ylabel("True Positive Rate",fontweight='bold')
plt.title("ROC Curve for Baseline model",fontweight='bold')
plt.plot([0,1],[0,1],color='red',linestyle='--')
plt.show()

"""**Predicting Politician Name**

Cross-validation for countvectorization
"""

#cross validation for Max_df
max_df_range = [0.4,0.5,0.6,0.7,0.8,0.9]


max_accuracy_LR = []
max_accuracy_KNN = []
max_accuracy_SVC = []

for max_df_value in max_df_range:
    print(max_df_value)
    count_vector = CountVectorizer(max_features=5000, ngram_range=(1,2), max_df = max_df_value)
    X_cross_features = count_vector.fit_transform(X_document)
    X_cross_features.todense()
    max_accuracy_LR.append(cross_val_LR(X_cross_features,y_person))
    max_accuracy_KNN.append(cross_val_KNN(X_cross_features,y_person))
    max_accuracy_SVC.append(cross_val_SVC(X_cross_features,y_person))

accuracy_errorbar(max_accuracy_LR,max_df_range,"Max_df", "Logistic Regression : Max_df")
accuracy_errorbar(max_accuracy_KNN,max_df_range,"Max_df", "KNN : Max_df")
accuracy_errorbar(max_accuracy_SVC,max_df_range,"Max_df", "SVC : Max_df")

#cross validation for Min_df
min_df_range = [0,0.1,0.2,0.3,0.4,0.5,0.6]
min_accuracy_LR = []
min_accuracy_KNN = []
min_accuracy_SVC = []

for min_df_value in min_df_range:
    print(min_df_value)
    count_vector = CountVectorizer(max_features=5000, ngram_range=(1,2), min_df = min_df_value)
    X_cross_features = count_vector.fit_transform(X_document)
    X_cross_features.todense()
    min_accuracy_LR.append(cross_val_LR(X_cross_features,y_person))
    min_accuracy_KNN.append(cross_val_KNN(X_cross_features,y_person))
    min_accuracy_SVC.append(cross_val_SVC(X_cross_features,y_person))

accuracy_errorbar(min_accuracy_LR,min_df_range,"Min_df", "Logistic Regression : Min_df")
accuracy_errorbar(min_accuracy_KNN,min_df_range,"Min_df", "KNN : Min_df")
accuracy_errorbar(min_accuracy_SVC,min_df_range,"Min_df", "SVC : Min_df")

#cross validation for Max_features
max_features_range = [100,500,1000,2000,3000,5000]


features_accuracy_LR = []
features_accuracy_KNN = []
features_accuracy_SVC = []

for max_features_value in max_features_range:
    print(max_features_value)
    count_vector = CountVectorizer(max_features=max_features_value, ngram_range=(1,2))
    X_cross_features = count_vector.fit_transform(X_document)
    X_cross_features.todense()
    features_accuracy_LR.append(cross_val_LR(X_cross_features,y_person))
    features_accuracy_KNN.append(cross_val_KNN(X_cross_features,y_person))
    features_accuracy_SVC.append(cross_val_SVC(X_cross_features,y_person))

accuracy_errorbar(features_accuracy_LR,max_features_range,"Max_features", "Logistic Regression : max_features")
accuracy_errorbar(features_accuracy_KNN,max_features_range,"Max_features", "KNN : max_features")
accuracy_errorbar(features_accuracy_SVC,max_features_range,"Max_features", "SVC : max_features")

#Cross-validation for ngram_range

max_ngram_range=[1,2,3]

ngram_accuracy_LR = []
ngram_accuracy_KNN = []
ngram_accuracy_SVC = []

for max_ngram_value in max_ngram_range:
    count_vector = CountVectorizer(max_features=2000, ngram_range=(1,max_ngram_value))
    X_cross_features = count_vector.fit_transform(X_document)
    X_cross_features.todense()
    ngram_accuracy_LR.append(cross_val_LR(X_cross_features,y_person))
    ngram_accuracy_KNN.append(cross_val_KNN(X_cross_features,y_person))
    ngram_accuracy_SVC.append(cross_val_SVC(X_cross_features,y_person))

accuracy_errorbar(ngram_accuracy_LR,max_ngram_range,"ngram", "Logistic Regression : ngram_range")
accuracy_errorbar(ngram_accuracy_KNN,max_ngram_range,"ngram", "KNN : ngram_range")
accuracy_errorbar(ngram_accuracy_SVC,max_ngram_range,"ngram", "SVC : ngram_range")

"""Model Selection"""

#Vectorizing our dataset
from sklearn.feature_extraction.text import CountVectorizer
count_vec = CountVectorizer(max_features = 5000, ngram_range = (1,1), min_df = 0, max_df=0.6)

X_features = count_vec.fit_transform(X_document)
X_features.todense()

#Error-bars for Logistic Regression C Value
from sklearn.linear_model import LogisticRegression

def error_bar_LR(X,y):

    C_range = [0.01,0.1,1,10,100]

    mean_accuracy=[]
    std_accuracy=[]

    for C_value in C_range:
        temp=[]
        for train,test in kf.split(X):
            model = LogisticRegression(C=C_value, max_iter=2000).fit(X[train],y[train])
            prediction = model.predict(X[test])
            temp.append(accuracy_score(y[test],prediction))
        mean_accuracy.append(np.array(temp).mean())
        std_accuracy.append(np.array(temp).std())

    #Error bar for Logistic Regression
    error_plots(C_range,mean_accuracy,std_accuracy,"C range","for Logistic Regression")
    error_plots(C_range[:4],mean_accuracy[:4],std_accuracy[:4],"C range","for Logistic Regression")

    print("For Logistic Regression\n\tbest value of C - {}\n\twith accuracy = {}".
          format(C_range[mean_accuracy.index(max(mean_accuracy))],max(mean_accuracy)))

error_bar_LR(X_features,y_person)

#Error bar for KNN
from sklearn.neighbors import KNeighborsClassifier

def error_bar_KNN(X,y):
    K_range = [2,3,4,5,6,7,8,9,10]

    mean_accuracy=[]
    std_accuracy=[]
    for K_value in K_range:
        temp=[]
        for train,test in kf.split(X):
            model = KNeighborsClassifier(n_neighbors=K_value).fit(X[train],y[train])
            prediction = model.predict(X[test])
            temp.append(accuracy_score(y[test],prediction))
        mean_accuracy.append(np.array(temp).mean())
        std_accuracy.append(np.array(temp).std())

    #Error bar for KNN
    error_plots(K_range,mean_accuracy,std_accuracy,"C range","for KNN")

    print("For KNN\n\tbest value of K - {}\n\twith accuracy = {}".
          format(K_range[mean_accuracy.index(max(mean_accuracy))],max(mean_accuracy)))

error_bar_KNN(X_features,y_person)

#Error bar for SVM C Value
from sklearn.svm import LinearSVC

def error_bar_SVC(X,y):

    C_range = [0.01,0.1,1,10,100]

    mean_accuracy=[]
    std_accuracy=[]

    for C_value in C_range:
        temp=[]
        for train,test in kf.split(X):
            model = LinearSVC(C=C_value,max_iter=1000).fit(X[train],y[train])
            prediction = model.predict(X[test])
            temp.append(accuracy_score(y[test],prediction))
        mean_accuracy.append(np.array(temp).mean())
        std_accuracy.append(np.array(temp).std())
    
    #Error bar for SVC
    error_plots(C_range,mean_accuracy,std_accuracy,"C range","for SVC")
    error_plots(C_range[:3],mean_accuracy[:3],std_accuracy[:3],"C range","for SVC")

    print("For SVC\n\tbest value of C - {}\n\twith accuracy = {}".
          format(C_range[mean_accuracy.index(max(mean_accuracy))],max(mean_accuracy)))

error_bar_SVC(X_features,y_person)

"""Training models"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

X_train,X_test,y_train,y_test = train_test_split(X_features,y_person,test_size=0.2)

#Logistic Regression

from sklearn.linear_model import LogisticRegression
LR_model = LogisticRegression(C=1, max_iter=2000)
LR_model.fit(X_train,y_train)

LR_predict = LR_model.predict(X_test)

print("Logistic Regression accuracy score : {}".format(accuracy_score(y_test,LR_predict)))
#LR_predict
#y_test

print("Confusion matrix for Logistic Regression model - ")
print(confusion_matrix(y_test,LR_predict))

#KNN
from sklearn.neighbors import KNeighborsClassifier
KNeighbor = KNeighborsClassifier(n_neighbors=2, weights='uniform')
KNeighbor.fit(X_train,y_train)
KNN_predict = KNeighbor.predict(X_test)
print("KNN : {}".format(accuracy_score(y_test,KNN_predict)))

print("Confusion matrix for KNN model - ")
print(confusion_matrix(y_test,KNN_predict))

#SVM
from sklearn.svm import LinearSVC

SVC_model = LinearSVC(C=0.1,max_iter=2000)
SVC_model.fit(X_train,y_train)

SVC_predict = SVC_model.predict(X_test)

print("SVC accuracy score : {}".format(accuracy_score(y_test,SVC_predict)))

print("Confusion matrix for SVC model - ")
print(confusion_matrix(y_test,SVC_predict))

#Did we over-fit

SVC_model = LinearSVC(C=1, max_iter=2000)
SVC_model.fit(X_train,y_train)

SVC_train_predict = SVC_model.predict(X_train)

print("SVC training accuracy : {}".format(accuracy_score(y_train,SVC_train_predict)))

#Dummy classifier
from sklearn.dummy import DummyClassifier
dummy = DummyClassifier(strategy="most_frequent")

dummy.fit(X_train,y_train)
dummy_prediction = dummy.predict(X_test)

print("Dummy classifier accuracy score : {}".format(accuracy_score(y_test,dummy_prediction)))

print("Confusion matrix for Dummy classifier - ")
print(confusion_matrix(y_test,dummy_prediction))

